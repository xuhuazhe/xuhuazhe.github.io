<!DOCTYPE html>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<link rel="stylesheet" type="text/css" href="./style/css">
	<link rel="stylesheet" type="text/css" href="./style/bootstrap.min.css">
	<link rel="stylesheet" type="text/css" href="./style/styles.css">
	<script src=""></script>
	<title><b>Huazhe Xu</b></title>
</head>

<body>

	<div class="my-container">
		<h1> <b>Huazhe Xu</b> </h1>
		<span>   <a href="https://scholar.google.com/citations?user=t9HPFawAAAAJ&hl=en">[Google Scholar]</a> <a href="https://twitter.com/HarryXu12">[Twitter]</a> </span>

		<div class="btn-group btn-group-justified" role="group" aria-label="Justified button group">
			<a href="index.html" class="btn btn-default" role="button">Home</a>
			<a href="publication.html" class="btn btn-default" role="button">Publication</a>
			<a href="group.html" class="btn btn-default" role="button">Group</a>
			<a href="contact.html" class="btn btn-default" role="button">Contact</a>
            <a href="misc.html" class="btn btn-default" role="button">Misc</a>
		</div>

		<div class="content">
			<div class="container">
                <p>( <sup>*</sup> indicates equally leading, <sup>+</sup> indicates equally advising)</p>

				<div class="row">

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/mentor.jpg" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Suning Huang<sup>*</sup>, Zheyu Zhang<sup>*</sup>, Tianhai Liang, Yihan Xu, Zhehao Kou, Chenhao Lu, Guowei Xu, Zhengrong Xue, <b>Huazhe Xu</b> <br> <i>MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2025.</p>
						<a href="https://arxiv.org/abs/2410.14972">[arXiv]</a>
						<a href="https://suninghuang19.github.io/mentor_page/">[project page]</a>
						<a href="https://x.com/suning_huang/status/1850219329331368313">[twitter]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/fog.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zilin Kang<sup>*</sup>, Chenyuan Hu<sup>*</sup>, Yu Luo, Zhecheng Yuan, Ruijie Zheng, Huazhe Xu <b>Huazhe Xu</b> <br> <i>A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2025.</p>
						<a href="https://openreview.net/pdf?id=VhmTXbsdtx">[paper]</a>
						<a href="">[project page coming soon]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/demogen.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhengrong Xue<sup>*</sup>, Shuying Deng<sup>*</sup>, Zhenyang Chen, Yixuan Wang, Zhecheng Yuan, <b>Huazhe Xu</b> <br> <i>DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning</i> <br>Robotics: Science and Systems <strong>(RSS)</strong>, 2025.</p>
						<a href="https://arxiv.org/abs/2502.16932">[arXiv]</a>
						<a href="https://demo-generation.github.io/">[project page]</a>
						<a href="https://github.com/TEA-Lab/DemoGen">[code]</a>
						<a href="https://www.bilibili.com/video/BV12cLczvE3N?buvid=Z9432A9D371832654C18B7171E33AF4EE126&from_spmid=main.space-contribution.0.0&is_story_h5=false&mid=MlQP42wHDkucPmxe20nGOQ%3D%3D&plat_id=116&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=0EBD6F9A-D3C2-40F2-96D0-D4B7AB41E6D1&share_source=COPY&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1745138938&unique_k=6f23B5B&up_id=3546888773044400&vd_source=088e6f4b756d22054c69902aea49d73f">[livestream record(in Chinese)]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/doglove.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Han Zhang, Songbo Hu, Zhecheng Yuan, <b>Huazhe Xu</b> <br> <i>DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove</i> <br>Robotics: Science and Systems <strong>(RSS)</strong>, 2025.</p>
						<a href="https://arxiv.org/pdf/2502.07730">[arXiv]</a>
						<a href="https://do-glove.github.io/">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/rdp.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Han Xue<sup>*</sup>, Jieji Ren<sup>*</sup>, Wendi Chen<sup>*</sup>, Gu Zhang, Yuan Fang, Guoying Gu,  <b>Huazhe Xu<sup>+</sup></b>,Cewu Lu<sup>+</sup> <br> <i>Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation</i> <br>Robotics: Science and Systems <strong>(RSS)</strong>, 2025.</p>
						<a href="https://arxiv.org/abs/2503.02881">[arXiv]</a>
						<a href="https://reactive-diffusion-policy.github.io/">[project page]</a>
						<a href="https://github.com/xiaoxiaoxh/reactive_diffusion_policy">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/morpheus.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zongzheng Zhang<sup>*</sup>, Jiawen Yang<sup>*</sup>, Ziqiao Peng, Meng Yang, Jianzhu Ma, Lin Cheng, <b>Huazhe Xu</b>, Hang Zhao, Hao Zhao <br> <i>Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control</i> <br>Robotics: Science and Systems <strong>(RSS)</strong>, 2025.</p>
						<a href="https://www.roboticsproceedings.org/rss21/p080.pdf">[arXiv]</a>
						<a href="https://github.com/ZZongzheng0918/Morpheus-Hardware">[hardware]</a>
						<a href="https://github.com/ZZongzheng0918/Morpheus-Software">[software coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/2by2.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yu Qi<sup>*</sup>, Yuanchen Ju<sup>*</sup>, Tianming Wei, Chi Chu, Lawson LS Wong, <b>Huazhe Xu</b> <br> <i>Two by Two: Learning multi-task pairwise objects assembly for generalizable robot manipulation</i> <br>Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2025.</p>
						<a href="https://arxiv.org/abs/2504.06961">[arXiv]</a>
						<a href="https://tea-lab.github.io/TwoByTwo/">[project page]</a>
						<a href="https://github.com/TEA-Lab/TwoByTWo">[code]</a>
						<a href="https://x.com/ju_yuanchen/status/1914692567062478951">[twitter]</a>
					</div>
				</div>

				<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/roboduet.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Guoping Pan<sup>*</sup>, Qingwei Ben<sup>*</sup>, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, <b>Huazhe Xu</b> <br> <i>RoboDuet: A Framework Affording
							Mobile-Manipulation and Cross-Embodiment</i> <br>IEEE Robotics and Automation Letters (<strong>RA-L</strong>) with a presentation at <strong>IROS'25</strong>.</p>
						<a href="https://arxiv.org/pdf/2403.17367.pdf">[arXiv]</a>
						<a href="https://locomanip-duet.github.io/">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/Stem-Ob.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Kaizhe Hu<sup>*</sup>, Zihang Rui<sup>*</sup>, Yao He, Yuyao Liu, Pu Hua, <b>Huazhe Xu</b> <br> <i>Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2025. (<strong style="color:red">spotlight</strong>)</p>
						<a href="https://hukz18.github.io/Stem-Ob/stem-ob.pdf">[arXiv]</a>
						<a href="https://hukz18.github.io/Stem-Ob/">[project page]</a>
						<a href="https://x.com/hkz222/status/1854780743685460235">[twitter]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/densematcher.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Junzhe Zhu<sup>*</sup>, Yuanchen Ju<sup>*</sup>, Junyi Zhang, Muhan Wang, Zhecheng Yuan, Kaizhe Hu, <b>Huazhe Xu</b> <br> <i>DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2025. (<strong style="color:red">spotlight</strong>)</p>
						<a href="https://arxiv.org/abs/2412.05268">[arXiv]</a>
						<a href="https://densematcher.github.io/">[project page]</a>
						<a href="https://x.com/ju_yuanchen/status/1865743859101188525">[twitter]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/mcr.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Guangqi Jiang<sup>*</sup>, Yifei Sun<sup>*</sup>, Tao Huang<sup>*</sup>, Huanyu Li, Yongyuan Liang<sup>+</sup>, <b>Huazhe Xu<sup>+</sup></b> <br> <i>Robots Pre-Train Robots: Manipulation- Centric Robotic Representation from Large- Scale Robot Datasets</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2025. </p>
						<a href="https://arxiv.org/pdf/2410.22325">[arXiv]</a>
						<a href="https://robots-pretrain-robots.github.io/">[project page]</a>
						<a href="https://x.com/LuccaChiang/status/1851651164187635732">[twitter]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/lbackward.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Haoran He, Can Chang, <b>Huazhe Xu</b>, Ling Pan <br> <i> Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2025. </p>
						<a href="https://arxiv.org/abs/2406.01150">[arXiv]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dllm.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zeyuan Liu<sup>*</sup>, Ziyu Huan<sup>*</sup>, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li<sup>+</sup>,
Furong Huang<sup>+</sup>,  <b>Huazhe Xu<sup>+</sup></b> <br> <i> World Models with Hints of Large Language Models
for Goal Achieving</i> <br> Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics <strong>(NAACL)</strong>, 2025. </p>
						<a href="https://arxiv.org/abs/2406.07381">[arXiv]</a>
					</div>
				</div>


				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dcmm.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yuanhang Zhang<sup>*</sup>, Tianhai Liang<sup>*</sup>, Zhenyang Chen, Yanjie Ze, <b>Huazhe Xu</b> <br> <i>Catch It! Learning to Catch in Flight with Mobile Dexterous Hands</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2025. (<strong style="color:red">outstanding paper nomination at CORL LFDM workshop</strong>)</p>
						<a href="https://arxiv.org/abs/2409.10319">[arXiv]</a>
						<a href="https://mobile-dex-catch.github.io/">[project page]</a>
						<a href="https://x.com/HarryXu12/status/1839890897657549281">[twitter]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/powersaving.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruiqian Nai, Jiacheng You, Liu Cao, Hanchen Cui, Shiyuan Zhang, <b>Huazhe Xu</b>, Yang Gao  <br> <i>Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2025. </p>
						<a href="https://hard-to-sim.github.io/static/root.pdf">[arXiv]</a>
						<a href="https://hard-to-sim.github.io/">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dtactive.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Jikai Xu<sup>*</sup>, Lei Wu<sup>*</sup>, Changyi Lin, Ding Zhao, and <b>Huazhe Xu</b> <br> <i>DTactive: A Vision-Based Tactile Sensor with Active Surface</i> <br>arXiv preprint, 2024.</p>
						<a href="https://arxiv.org/abs/2410.08337">[arXiv]</a>
						<a href="https://ieqefcr.github.io/DTactive/">[project page]</a>
						<a href="https://drive.google.com/file/d/1QzzIjNFbDpvQvcflzC-6dMTOlpp0q9Jp/view">[hardware]</a>
						<a href="https://x.com/ieqefcr/status/1845663779012723115">[twitter]</a>
					</div>
				</div>


				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/3dkp.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Chengkai Hou, Zhengrong Xue, Bingyang Zhou, Jinghan Ke, Shao Lin, <b>Huazhe Xu</b> <br> <i>Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap Features</i> <br>Conference on Neural Information Processing Systems Datasets & Benchmarks Track <strong>(NeurIPS)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2410.02237">[arXiv]</a>
						<a href="https://jackhck.github.io/keygrid.github.io/">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/maa.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, <b>Huazhe Xu</b> <br> <i>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</i> <br>Conference on Neural Information Processing Systems Datasets & Benchmarks Track <strong>(NeurIPS)</strong>, 2024.</p>
						<a href="https://arxiv.org/pdf/2407.10973">[arXiv]</a>
						<a href="https://cheryyunl.github.io/make-an-agent/">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/maniwhere.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhecheng Yuan<sup>*</sup>, Tianming Wei<sup>*</sup>, Shuiqi Cheng, Gu Zhang, Yuanpei Chen, <b>Huazhe Xu</b> <br> <i>Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning</i> <br>Conference on Robot Learning <strong>(CoRL)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2407.15815">[arXiv]</a>
						<a href="https://gemcollector.github.io/maniwhere/">[project page]</a>
						<a href="https://github.com/gemcollector/maniwhere">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/riemann.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Chongkai Gao, Zhengrong Xue, Shuying Deng, Tianhai Liang, Siqi Yang, Lin Shao, <b>Huazhe Xu</b> <br> <i>RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation</i> <br>Conference on Robot Learning <strong>(CoRL)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2403.19460">[arXiv]</a>
						<a href="https://riemann-web.github.io/">[project page]</a>
						<a href="https://github.com/HeegerGao/RiEMann">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/gensim2.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Pu Hua<sup>*</sup>, Minghuan Liu<sup>*</sup>, Annabella Macaluso<sup>*</sup>, Yunfeng Lin, Weinan Zhang, <b>Huazhe Xu</b>, Lirui Wang <br> <i>GenSim2: Scaling Robotic Data Generation with Multi-modal and Reasoning LLMs</i> <br>Conference on Robot Learning <strong>(CoRL)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2410.03645">[arXiv]</a>
						<a href="https://gensim2.github.io/">[project page]</a>
						<a href="https://github.com/GenSim2/gensim2">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/roboabc.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yuanchen Ju<sup>*</sup>, Kaizhe Hu<sup>*</sup>, Guowei Zhang, Gu Zhang, Mingrun Jiang, <b>Huazhe Xu</b> <br> <i>Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation</i> <br>European Conference on Computer Vision <strong>(ECCV)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2401.07487">[arXiv]</a>
						<a href="https://tea-lab.github.io/Robo-ABC/">[project page]</a>
						<a href="https://github.com/TEA-Lab/Robo-ABC">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/diffusion-reward.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Tao Huang<sup>*</sup>, Guangqi Jiang<sup>*</sup>, Yanjie Ze, <b>Huazhe Xu</b> <br> <i>Diffusion Reward: Learning Rewards via Conditional Video Diffusion</i> <br>European Conference on Computer Vision <strong>(ECCV)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2312.14134">[arXiv]</a>
						<a href="https://diffusion-reward.github.io/">[project page]</a>
						<a href="https://github.com/TaoHuang13/diffusion_reward">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/locoman.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, <b>Huazhe Xu</b> <br> <i>Learning Visual Quadrupedal Loco-Manipulation from Demonstrations</i> <br>International Conference on Intelligent Robots and Systems <strong>(IROS)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2403.20328">[arXiv]</a>
						<a href="https://zhengmaohe.github.io/leg-manip/">[project page]</a>
						<a href="https://twitter.com/ZhengmaoHe/status/1774842311765049698">[Twitter/X]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<!--<div class="row">-->
					<!--<div class="media col-md-3">-->
						<!--<figure class="pull-left">-->
							<!--<img class="media-object img-rounded img-responsive"  src="images/pub-pic/m-a-a.gif" >-->
						<!--</figure>-->
					<!--</div>-->
					<!--<div class="col-md-8">-->
						<!--<p> Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, <b>Huazhe Xu</b> <br> <i>Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion</i> <br>ArXiv Preprint, 2024.</p>-->
						<!--<a href="https://arxiv.org/pdf/2407.10973">[arXiv]</a>-->
						<!--<a href="https://cheryyunl.github.io/make-an-agent/">[project page]</a>-->
						<!--<a href="https://github.com/cheryyunl/Make-An-Agent">[code]</a>-->
						<!--<a href="https://x.com/cheryyun_l/status/1813226234979184687">[twitter]</a>-->
					<!--</div>-->
				<!--</div>-->



				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/hint.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, <b>Huazhe Xu</b> <br> <i>World Models with Hints of Large Language Models for Goal Achieving</i> <br>ArXiv Preprint, 2024.</p>
						<a href="https://arxiv.org/abs/2406.07381">[arXiv]</a>
					</div>
				</div>


				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dp3.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yanjie Ze<sup>*</sup>, Gu Zhang<sup>*</sup>, Kangning Zhang, Chenyuan Hu, Muhan Wang, <b>Huazhe Xu</b> <br> <i>3D Diffusion Policy</i> <br>Robotics: Science and Systems <strong>(RSS)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2403.03954">[arXiv]</a>
						<a href="https://3d-diffusion-policy.github.io/">[project page]</a>
						<a href="https://github.com/YanjieZe/3D-Diffusion-Policy">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/causal.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Tianying Ji<sup>*</sup>, Yongyuan Liang<sup>*</sup>, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, <b>Huazhe Xu</b> <br> <i>ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024. (<strong style="color:red">oral</strong>)</p>
						<a href="https://arxiv.org/abs/2402.14528">[arXiv]</a>
						<a href="https://ace-rl.github.io/">[project page]</a>
						<a href="https://github.com/jity16/ACE-Off-Policy-Actor-Critic-with-Causality-Aware-Entropy-Regularization">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/ompo.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yu Luo, Tianying Ji, Fuchun Sun, Jianwei Zhang, <b>Huazhe Xu</b>, Xianyuan Zhan  <br> <i>OMPO: A Unified Framework for RL under Policy and Dynamics Shifts</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024. (<strong style="color:red">oral</strong>)</p>
						<a href="https://arxiv.org/pdf/2405.19080">[arXiv]</a>
						<a href="https://github.com/Roythuly/OMPO">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/OBAC.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yu Luo, Tianying Ji, Fuchun Sun, Jianwei Zhang, <b>Huazhe Xu</b>, Xianyuan Zhan  <br> <i>Offline-Boosted Actor-Critic:Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024. </p>
						<a href="https://arxiv.org/abs/2405.18520">[arXiv]</a>
						<a href="https://roythuly.github.io/OBAC_web/">[project page]</a>
						<a href="https://github.com/Roythuly/OBAC">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/rethinking.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Chenhao Lu, Ruizhe Shi, Yuyao Liu, Kaizhe Hu, Simon S. Du, <b>Huazhe Xu</b>  <br> <i>Rethinking Transformers in Solving POMDPs</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024. </p>
						<a href="https://arxiv.org/abs/2405.17358">[arXiv]</a>
						<a href="https://github.com/CTP314/TFPORL">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/bac.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Tianying Ji, Yu Luo, Fuchun Sun, Xianyuan Zhan, Jianwei Zhang, <b>Huazhe Xu</b>. <br> <i>Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024.</p>
						<a href="https://arxiv.org/pdf/2306.02865.pdf">[arXiv]</a>
						<a href="https://jity16.github.io/BEE/">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/ptaco.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daumé III, <b>Huazhe Xu</b>, John Langford, Praveen Palanisamy, Kalyan Shankar Basu, Furong Huang. <br> <i>Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2024.</p>
						<a href="https://arxiv.org/pdf/2402.06187">[arXiv]</a>
						<a href="https://premiertaco.github.io/">[project page]</a>
						<a href="https://github.com/PremierTACO/premier-taco">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/arraybot.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhengrong Xue<sup>*</sup>, Han Zhang<sup>*</sup>, Jingwen Cheng, Zhengmao He, Yuanchen Ju, Changyi Lin, Gu Zhang, <b>Huazhe Xu</b> <br> <i>ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch
</i> <br>International Conference on Robot Automation <strong>(ICRA)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2306.16857">[arXiv]</a>
						<a href="https://steven-xzr.github.io/ArrayBot/">[project page]</a>
						<a href="https://github.com/Steven-xzr/ArrayBot">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/hirohand.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Dehao Wei, <b>Huazhe Xu</b> <br> <i>A Wearable Robotic Hand for Hand-over-Hand Imitation Learning</i> <br>International Conference on Robot Automation <strong>(ICRA)</strong>, 2024.</p>
						<a href="https://arxiv.org/pdf/2309.14860.pdf">[arXiv]</a>
						<a href="https://sites.google.com/view/hiro-hand/%E9%A6%96%E9%A1%B5">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/deformnet.jpg" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Chenchang Li<sup>*</sup>, Zihao Ai<sup>*</sup>, Tong Wu, Xiaosa Li, Wenbo Ding<sup>+</sup>, <b>Huazhe Xu<sup>+</sup></b> <br> <i>DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable Object Manipulation
</i> <br>International Conference on Robot Automation <strong>(ICRA)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2402.07648">[arXiv]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/drm.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Guowei Xu<sup>*</sup>, Ruijie Zheng<sup>*</sup>, Yongyuan Liang<sup>*</sup>, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, <b>Huazhe Xu</b> <br> <i>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024. (<strong style="color:red">spotlight, top 5%</strong>)</p>
						<a href="https://arxiv.org/abs/2310.19668">[arXiv]</a>
						<a href="https://xugw-kevin.github.io/drm/">[project page]</a>
						<a href="https://github.com/XuGW-Kevin/DrM">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/gensim.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Lirui Wang, Yiyang Ling<sup>*</sup>, Zhecheng Yuan<sup>*</sup>, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, <b>Huazhe Xu</b>, Xiaolong Wang <br> <i>GenSim: Generating Robotic Simulation Tasks via Large Language Models</i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024. (<strong style="color:red">spotlight, top 5%</strong>)<br>Abridged version in LangRob Workshop@CoRL'23 (<strong style="color:red">outstanding paper award</strong>).</p>
						<a href="https://arxiv.org/abs/2310.01361">[arXiv]</a>
						<a href="https://liruiw.github.io/gensim/">[project page]</a>
						<a href="https://github.com/liruiw/GenSim">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/unio4.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Kun Lei, Zhengmao He<sup>*</sup>, Chenhao Lu<sup>*</sup>, Kaizhe Hu, Yang Gao, <b>Huazhe Xu</b> <br> <i>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step Policy Optimization</i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2311.03351">[arXiv]</a>
						<a href="https://lei-kun.github.io/uni-o4/">[project page]</a>
						<a href="https://github.com/Lei-Kun/Uni-o4">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/LaMo.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruizhe Shi<sup>*</sup>, Yuyao Liu<sup>*</sup>, Yanjie Ze, Simon S. Du, <b>Huazhe Xu</b> <br> <i>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2310.20587">[arXiv]</a>
						<a href="https://lamo2023.github.io/">[project page]</a>
						<a href="https://github.com/srzer/LaMo-2023">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/coplanner.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, <b>Huazhe Xu</b>, Furong Huang
 <br> <i>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024.</p>
						<a href="https://arxiv.org/abs/2310.07220">[arXiv]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/morphmaze.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Suning Huang, Boyuan Chen, <b>Huazhe Xu</b>, Vincent Sitzmann
 <br> <i> Morphological Maze: Control Reconfigurable Soft Robots with Fine-grained Morphology Change </i> <br>International Conference on Learning Representation <strong>(ICLR)</strong>, 2024.</p>
						<a href="https://openreview.net/pdf?id=MpyFAhH9CK">[paper]</a>
						<a href="https://dittogym.github.io/">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/SAM-G.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ziyu Wang<sup>*</sup>, Yanjie Ze<sup>*</sup>, Yifei Sun, Zhecheng Yuan, <b>Huazhe Xu</b> <br> <i>Generalizable Visual Reinforcement Learning with Segment Anything Model</i> <br>arXiv preprint.</p>
						<a href="https://arxiv.org/abs/2312.17116">[arXiv]</a>
						<a href="https://yanjieze.com/SAM-G/">[project page]</a>
						<a href="https://github.com/wadiuvatzy/SAM-G">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/9dtact.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Changyi Lin, Han Zhang, Jikai Xu, Lei Wu, <b>Huazhe Xu</b> <br> <i>9DTact: A Compact Vision-Based Tactile Sensor for Accurate 3D Shape Reconstruction and Generalizable 6D Force Estimation</i> <br>IEEE Robotics and Automation Letters <strong>(RA-L)</strong> with a presentation at <strong>ICRA'24</strong>.</p>
						<a href="https://arxiv.org/pdf/2308.14277.pdf">[arXiv]</a>
						<a href="https://linchangyi1.github.io/9DTact/">[project page]</a>
						<a href="https://github.com/linchangyi1/9DTact">[hardware & code]</a>
						<a href="https://www.bilibili.com/video/BV1nu411w7t4/">[tutorial[CN]]</a>
						<a href="https://youtu.be/VxMceWVz7QQ?si=SwPTC4h75ZeSAkrT">[tutorial[EN]]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/robocraft3D.jpeg" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Haochen Shi<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Zhiao Huang, Yunzhu Li, Jiajun Wu. <br>
							<i> RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects in 3D with Graph Networks </i> <br> International Journal of Robotics Research <strong>(IJRR)</strong>.(<strong style="color:red">acceptance rate: 5.3%; IF: 6.887</strong>)</p>
						<a href="https://journals.sagepub.com/doi/10.1177/02783649231219020">[paper]</a>

					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/vigen.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhecheng Yuan<sup>*</sup>, Sizhe Yang<sup>*</sup>, Pu Hua, Can Chang, Kaizhe Hu, <b>Huazhe Xu</b> <br> <i>RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization</i> <br>Conference on Neural Information Processing Systems Datasets & Benchmarks Track <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="https://arxiv.org/abs/2307.10224">[arXiv]</a>
						<a href="https://gemcollector.github.io/RL-ViGen/">[project page]</a>
						<a href="https://github.com/gemcollector/RL-ViGen">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/taco.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, <b>Huazhe Xu</b><sup>+</sup>, Hal Daumé III<sup>+</sup>, Furong 	Huang<sup>+</sup><br> <i>TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning</i> <br>Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="http://arxiv.org/abs/2306.13229">[arXiv]</a>
						<a href="https://ruijiezheng.com/project/TACO/index.html">[project page]</a>
						<a href="https://github.com/FrankZheng2022/TACO">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/DM_RL.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Jialu Gao<sup>*</sup>, Kaizhe Hu<sup>*</sup>, Guowei Xu, <b>Huazhe Xu</b> <br> <i>Can Pre-Trained Text-to-Image Models Generate
Visual Goals for Reinforcement Learning?</i> <br>Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="https://arxiv.org/pdf/2307.07837.pdf">[arXiv]</a>
						<a href="https://lfvoid-rl.github.io/">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/MoVie.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Sizhe Yang<sup>*</sup>, Yanjie Ze<sup>*</sup>, <b>Huazhe Xu</b> <br> <i>MoVie: Visual Model-Based Policy Adaptation
for View Generalization</i> <br>Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="https://arxiv.org/pdf/2307.00972.pdf">[arXiv]</a>
						<a href="https://yangsizhe.github.io/MoVie/">[project page]</a>
						<a href="https://github.com/yangsizhe/MoVie">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/h-index.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yanjie Ze, Yuyao Liu<sup>*</sup>, Ruizhe Shi<sup>*</sup>, Jiaxin Qin, Zhecheng Yuan, Jiashun Wang, <b>Huazhe Xu</b> <br> <i>H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation</i> <br>Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="https://arxiv.org/pdf/2310.01404.pdf">[arXiv]</a>
						<a href="https://yanjieze.com/H-InDex/">[project page]</a>
						<a href="https://github.com/YanjieZe/H-InDex">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/ceil.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Jinxin Liu<sup>*</sup>, Li He<sup>*</sup>, Yachen Kang, Zifeng Zhuang, Donglin Wang, <b>Huazhe Xu</b> <br> <i>CEIL: Generalized Contextual Imitation Learning</i> <br>Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2023.</p>
						<a href="https://arxiv.org/pdf/2306.14534v1.pdf">[arXiv]</a>
						<a href="https://github.com/wechto/GeneralizedCEIL">[code]</a>
					</div>
				</div>



				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/robocook.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Haochen Shi<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Samuel Clarke, Yunzhu Li, Jiajun Wu <br> <i>RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</i> <br>Conference on Robot Learning <strong>(CoRL)</strong>, 2023. (<strong style="color:red">best system paper award</strong>, <strong style="color:red">oral</strong>)</p>
						<a href="https://arxiv.org/abs/2306.14447">[arXiv]</a>
						<a href="https://hshi74.github.io/robocook/">[project page]</a>
						<a href="https://github.com/hshi74/robocook">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/otas.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yuerong Li, Zhengrong Xue, <b>Huazhe Xu</b> <br> <i>OTAS: Unsupervised Boundary Detection for Object-Centric
Temporal Action Segmentation</i> <br>IEEE/CVF Winter Conference on Applications of Computer Vision <strong>(WACV)</strong>, 2024. </p>
						<a href="https://arxiv.org/pdf/2309.06276.pdf">[arXiv]</a>
						<a href="https://github.com/yl596/OTAS">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/no_pretrain.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Nicklas Hansen<sup>*</sup>, Zhecheng Yuan<sup>*</sup>, Yanjie Ze<sup>*</sup>, Tongzhou Mu<sup>*</sup>, Aravind Rajeswaran<sup>+</sup>, Hao Su<sup>+</sup>, <b>Huazhe Xu</b><sup>+</sup>, Xiaolong Wang<sup>+</sup>. <br> <i>On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline</i> <br>International Conference on Machine Learning <strong>(ICML)</strong>, 2023.</p>
						<a href="https://arxiv.org/pdf/2212.05749.pdf">[arXiv]</a>
						<a href="https://github.com/gemcollector/learning-from-scratch">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/useek.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhengrong Xue, Zhecheng Yuan, Jiashun Wang, Xueqian Wang, Yang Gao, <b>Huazhe Xu</b>. <br> <i>USEEK: Unsupervised SE(3)-Equivariant 3D Keypoints for Generalizable Manipulation</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2023.  </p>
						<a href="https://arxiv.org/abs/2209.13864">[arXiv]</a>
						<a href="https://sites.google.com/view/useek/">[project page]</a>
						<a href="https://github.com/Steven-xzr/USEEK">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dtact.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Changyi Lin, Ziqi Lin, Shaoxiong Wang, <b>Huazhe Xu</b>. <br> <i>DTact: A Vision-Based Tactile Sensor that Measures High-Resolution 3D Geometry Directly from Darkness</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2023.  </p>
						<a href="https://arxiv.org/abs/2209.13916">[arXiv]</a>
						<a href="https://sites.google.com/view/dtact-sensor">[project page]</a>
						<a href="https://www.technology.org/2022/10/01/dtact-a-vision-based-tactile-sensor-that-measures-high-resolution-3d-geometry-directly-from-darkness/">[media]</a>
						<a href="https://github.com/linchangyi1/DTact">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/eil.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ray Chen Zheng<sup>*</sup>, Kaizhe Hu<sup>*</sup>, Zhecheng Yuan, Boyuan Chen, <b>Huazhe Xu</b>. <br> <i>Extraneousness-Aware Imitation Learning</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2023.  </p>
						<a href="https://arxiv.org/abs/2210.01379">[arXiv]</a>
						<a href="https://sites.google.com/view/eil-website">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/bihandreal.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yunfei Li<sup>*</sup>, Chaoyi Pan<sup>*</sup>, <b>Huazhe Xu</b>, Xiaolong Wang, Yi Wu. <br> <i>Efficient Bimanual Handover and Rearrangement via Symmetry-Aware Actor-Critic Learning</i> <br> International Conference on Robot Automation <strong>(ICRA)</strong>, 2023.  </p>
						<a href="https://drive.google.com/file/d/1ISJ2yjqeuiUgA4wzdI3Gbi_AN5k_RKGP/view">[paper]</a>
						<a href="https://panchaoyi.com/efficient-bimanual-handover-and-rearrangement-via-symmetry-aware-actor-critic-learning">[project page]</a>
						<a href="https://github.com/jc-bao/srl">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/defog_demo.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Kaizhe Hu<sup>*</sup>, Ray Zheng<sup>*</sup>, Yang Gao, <b>Huazhe Xu</b>. <br> <i>Decision Transformer under Random Frame Dropping</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2023. </p>
						<a href="https://arxiv.org/abs/2303.03391">[arXiv]</a>
						<a href="https://github.com/hukz18/DeFog">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/sear2.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Pu Hua, Yubei Chen<sup>+</sup>, <b>Huazhe Xu</b><sup>+</sup>. <br> <i>Simple Emergent Action Representations
from Multi-Task Policy Training</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2023. </p>
						<a href="https://arxiv.org/abs/2210.09566">[arXiv]</a>
						<a href="https://sites.google.com/view/emergent-action-representation/">[project page]</a>
						<a href="https://github.com/piao-0429/EAR">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/implicit.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Linfeng Zhao, <b>Huazhe Xu</b>, Lawson L.S. Wong. <br> <i>Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2023. </p>
						<a href="https://arxiv.org/abs/2210.13542">[arXiv]</a>
						<a href="https://github.com/zhao0625/DiffPlan">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/vaml.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruijie Zheng<sup>*</sup>, Xiyao Wang<sup>*</sup>, <b>Huazhe Xu</b>, Furong Huang. <br> <i>Is Model Ensemble Necessary? Model-based RL via a Single Model with Lipschitz Regularized Value Function</i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2023.  </p>
						<p> Abridged in NeurIPS 2022 DRL workshop (<strong style="color:red">spotlight</strong>). </p>
						<a href="https://arxiv.org/abs/2302.01244">[arXiv]</a>
						<a href="https://ruijiezheng.com/project/mbrl_lipschitz/index.html">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/pieg.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, <b>Huazhe Xu</b>. <br> <i>Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning</i> <br> Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2022. (<strong style="color:red">spotlight</strong>)  </p>
						<a href="https://arxiv.org/abs/2212.08860">[arxiv]</a>
						<a href="https://sites.google.com/view/pie-g/home">[project page]</a>
						<a href="https://github.com/gemcollector/PIE-G">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/emapp.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Can Chang, Ni Mu, Jiajun Wu, Ling Pan, <b>Huazhe Xu</b>. <br> <i>E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance</i> <br> Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2022. (<strong style="color:red">spotlight</strong>) </p>
						<a href="https://arxiv.org/abs/2212.02064">[arXiv]</a>
						<a href="https://sites.google.com/view/e-mapp">[project page]</a>
						<a href="https://github.com/canchang00/e_mapp">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/robotube.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, <b>Huazhe Xu</b>, Cewu Lu <br> <i>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments </i> <br>  RSS Workshop, 2022.  </p>
						<a href="https://openreview.net/pdf?id=SYUEnQtK85o">[openreview]</a>
						<a href="https://www.youtube.com/watch?v=oTnsZs7yOVc">[video]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/sht.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Hao Li, Yizhi Zhang, Junzhe Zhu, Shaoxiong Wang, Michelle A Lee, <b>Huazhe Xu</b>, Edward Adelson, Li Fei-Fei, Ruohan Gao, Jiajun Wu. <br> <i>See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation </i> <br>  International Conference on Robots Learning <strong>(CORL)</strong>, 2022.  </p>
						<a href="https://arxiv.org/pdf/2212.03858.pdf">[arXiv]</a>
						<a href="https://ai.stanford.edu/~rhgao/see_hear_feel/">[project page]</a>
						<a href="https://github.com/JunzheJosephZhu/see_hear_feel">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/summon.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yixing Wang<sup>*</sup>, Sifan Ye<sup>*</sup>, Jiaman Li, Dennis Lee, Karen Liu, <b>Huazhe Xu</b><sup>+</sup>, Jiajun Wu<sup>+</sup>. <br> <i>Scene Synthesis from Human Motion </i> <br> <strong>Siggraph Asia</strong>, 2022.  </p>
						<a href="https://arxiv.org/pdf/2301.01424.pdf">[arXiv]</a>
						<a href="https://lijiaman.github.io/projects/summon/">[project page]</a>
						<a href="https://github.com/onestarYX/summon">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/piano_robot2.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> <b>Huazhe Xu</b>, Yuping Luo, Shaoxiong Wang, Trevor Darrell, Roberto Calandra. <br> <i>Towards Learning to Play Piano with Dexterous Hands and Touch </i> <br>  International Conference on Intelligent Robots and Systems <strong>(IROS)</strong>, 2022.  </p>
						<a href="https://arxiv.org/abs/2106.02040">[arXiv]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/omarsc2.gif" >
						</figure>
					</div>
				<div class="col-md-8">
						<p> Ling Pan, Longbo Huang, Tengyu Ma, <b>Huazhe Xu</b>. <br> <i> Plan Better amid Conservatism: Offline Multi-agent Reinforcement Learning with Actor Rectification </i> <br> International Conference on Machine Learning <strong>(ICML)</strong>, 2022.</p>
						<a href="https://arxiv.org/abs/2111.11188">[arXiv]</a>
						<a href="https://sites.google.com/view/omar-videos">[project page]</a>
						<a href="https://github.com/ling-pan/OMAR">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/phasic_rl.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yunfei Li, Tian Gao, Jiaqi Yang, <b>Huazhe Xu</b>, Yi Wu. <br> <i> Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning </i> <br> International Conference on Machine Learning <strong>(ICML)</strong>, 2022.</p>
						<a href="http://arxiv.org/abs/2206.12030">[arXiv]</a>
						<a href="https://sites.google.com/view/pair-gcrl">[project page]</a>
						<a href="https://github.com/IrisLi17/onpolicy_algorithm">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/robocraft.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Haochen Shi<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Zhiao Huang, Yunzhu Li, Jiajun Wu. <br>
							<i> RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks </i> <br> Robotics: Science and Systems <strong>(RSS)</strong>, 2022.</p>

						<a href="https://arxiv.org/pdf/2205.02909.pdf">[arXiv]</a>
						<a href="http://hxu.rocks/robocraft">[project page]</a>
						<a href="https://github.com/hshi74/robocraft">[code]</a>
						<a href="https://news.mit.edu/2022/robots-play-play-dough-0623">[media MIT Tech Review]</a>
						<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh">[media HAI]</a>
						<!--<a href="https://www.youtube.com/watch?v=viBArg6QIQQ&feature=youtu.be">[video]</a>-->
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/tlda.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, <b>Huazhe Xu</b>. <br> <i>Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning </i> <br> International Joint Conference on Artificial Intelligence <strong>(IJCAI)</strong>, 2022.</p>
						<a href="https://arxiv.org/pdf/2202.09982">[arXiv]</a>
						<a href="https://sites.google.com/view/algotlda/home">[project page]</a>
						<!--<a href="https://www.youtube.com/watch?v=viBArg6QIQQ&feature=youtu.be">[video]</a>-->
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/dog.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Ruihan Yang<sup>*</sup>, Minghao Zhang<sup>*</sup>, Nicklas Hansen, <b>Huazhe Xu</b>, Xiaolong Wang. <br> <i>Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers </i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2022. (<strong style="color:red">spotlight</strong>)</p>
						<a href="https://arxiv.org/abs/2107.03996">[arXiv]</a>
						<a href="https://rchalyang.github.io/LocoTransformer/">[project page]</a>
						<a href="https://www.youtube.com/watch?v=viBArg6QIQQ&feature=youtu.be">[video]</a>
						<a href="https://github.com/Mehooz/vision4leg">[code]</a>

					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/mp3d.jpeg" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Jiashun Wang, <b>Huazhe Xu</b>, Medhini Narasimhan, Xiaolong Wang. <br> <i> Multi-Person 3D Motion Prediction with Multi-Range Transformers. </i> <br> Conference on Neural Information Processing Systems  <strong>(NeurIPS)</strong>, 2021.</p>
						<a href="https://arxiv.org/abs/2111.12073">[arXiv]</a>
						<a href="https://jiashunwang.github.io/MRT/">[project page]</a>
						<a href="https://github.com/jiashunwang/MRT">[code]</a>
					</div>
				</div>


				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/bebold.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Tianjun Zhang, <b>Huazhe Xu</b>, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E. Gonzalez, Yuandong Tian. <br> <i>NovelD: A Simple yet Effective Exploration Criterion </i>  <br> Conference on Neural Information Processing Systems  <strong>(NeurIPS)</strong>, 2021.</p>
						<a href="https://arxiv.org/abs/2012.08621">[arXiv]</a>
						<a href="https://github.com/tianjunz/NovelD">[code]</a>
						<a href="https://slideslive.com/38941271/bebold-exploration-beyond-the-boundary-of-explored-regions?ref=account-folder-62083-folders">[talk]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/bimanual.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Minghao Zhang<sup>*</sup>, Pingcheng Jian<sup>*</sup>, Yi Wu, <b>Huazhe Xu</b>, Xiaolong Wang. <br> <i>Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation </i> <br>  Arxiv Preprints. </p>
						<a href="https://arxiv.org/abs/2106.05907">[arXiv]</a>
						<a href="https://mehooz.github.io/bimanual-attention/">[project page]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>


				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/sap2.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> <b>Huazhe Xu</b><sup>*</sup>, Boyuan Chen<sup>*</sup>, Yang Gao, Trevor Darrell. <br> <i>Zero-shot  Policy  Learning  with  Spatial  Temporal  Reward Decomposition  on  Contingency-aware  Observation </i> <br>  International Conference on Robot Automation <strong>(ICRA)</strong>, 2021. </p>
						<a href="https://arxiv.org/abs/1910.08143">[arXiv]</a>
						<a href="https://github.com/buoyancy99/sap">[code]</a>
						<a href="https://sites.google.com/view/sapnew/home">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/pytouch.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Mike Lambeta, <b>Huazhe Xu</b>, Jingwei Xu, Po-Wei Chou, Shaoxiong Wang,Trevor Darrell, and Roberto Calandra <br> <i>PyTouch:  A  Machine  Learning  Library  for  Touch  Processing </i> <br>  International Conference on Robot Automation <strong>(ICRA)</strong>, 2021. </p>
						<a href="https://arxiv.org/abs/2105.12791">[arXiv]</a>
						<a href="https://github.com/facebookresearch/pytouch">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/motion3d.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Jiashun Wang, <b>Huazhe Xu</b>, Jingwei Xu, Sifei Liu, Xiaolong Wang. <br> <i>Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes. </i>  <br> Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2021. </p>
						<a href="https://arxiv.org/abs/2012.05522">[arXiv]</a>
						<a href="https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes/">[project page]</a>
						<a href="https://github.com/jiashunwang/Long-term-Motion-in-3D-Scenes">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/sir.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Yunfei Li, <b>Huazhe Xu</b>, Yilin Wu, Xiaolong Wang, Yi Wu. <br> <i>Solving Compositional Reinforcement Learning Problems via Task Reduction. </i>  <br> International Conference on Learning Representations <strong>(ICLR)</strong>, 2021.</p>
						<a href="https://openreview.net/pdf?id=9SS69KwomAM">[openreview]</a>
						<a href="https://arxiv.org/abs/2103.07607">[arxiv]</a>
						<a href="https://github.com/IrisLi17/self-imitation-via-reduction">[code]</a>
						<a href="https://sites.google.com/view/sir-compositional">[project page]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/rpg.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Zhenggang Tang, Chao Yu, Boyuan Chen, <b>Huazhe Xu</b>, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, Yi Wu. <br> <i>Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization. </i>  <br> International Conference on Learning Representations <strong>(ICLR)</strong>, 2021.</p>
						<a href="https://openreview.net/pdf?id=lvRTC669EY">[openreview]</a>
						<a href="https://sites.google.com/view/staghuntrpg">[project page]</a>
						<a href="https://github.com/staghuntrpg/RPG">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/collaq.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p> Tianjun Zhang, <b>Huazhe Xu</b>, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E. Gonzalez, Yuandong Tian. <br> <i>Multi-Agent Collaboration via Reward Attribution Decomposition </i>  <br> Arxiv preprints, 2020.</p>
						<a href="https://arxiv.org/abs/2010.08531">[arXiv]</a>
						<a href="https://github.com/facebookresearch/CollaQ">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/soft.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Ruihan Yang, <b>Huazhe Xu</b>, Yi Wu, Xiaolong Wang. <br> <i>Multi-Task Reinforcement Learning with Soft Modularization. </i>  <br> Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2020.</p>
						<a href="https://proceedings.neurips.cc/paper/2020/file/32cfdce9631d8c7906e8e9d6e68b514b-Paper.pdf">[pdf]</a>
						<a href="https://github.com/RchalYang/Soft-Module">[code]</a>
						<a href="https://rchalyang.github.io/SoftModule/">[project page]</a>
						<a href="http://www.betr-rl.ml/2020/abs/18/">[Talk]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/motion.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Jingwei Xu<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Bingbing Ni, Xiaokang Yang, Xiaolong Wang, Trevor Darrell. <br> <i>Hierarchical Style-based Networks for Motion Synthesis. </i> <br> European Conference on Computer Vision <strong>(ECCV)</strong>, 2020. </p>
						<a href="https://arxiv.org/abs/2008.10162">[arXiv]</a>
						<a href="https://sites.google.com/view/hsnms">[project page]</a>
						<a href="bibs/motionsyn.txt">[BibTeX]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/vpeg.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Jingwei Xu<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Bingbing Ni, Xiaokang Yang, Trevor Darrell. <br> <i>Video Prediction via Demonstration Guidance </i> <br> International Conference on Machine Learning <strong>(ICML)</strong>, 2020. (<strong style="color:red">oral</strong>) </p>
						<a href="https://arxiv.org/pdf/2007.01738">[arXiv]</a>
						<a href="https://sites.google.com/view/vpeg-supp/home">[project page]</a>
						<a href="https://github.com/xjwxjw/VPEG">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/ilns.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Yuping Luo, <b>Huazhe Xu</b>, Tengyu Ma. <br> <i>Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling, </i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2020. </p>
						<a href="https://arxiv.org/abs/1907.05634">[arXiv]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/unsup_driving.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Jierui Lin<sup>*</sup>, Yifei Xing<sup>*</sup>, <b>Huazhe Xu</b>, Yang Gao. <br> <i>Learning a Perception-Logic Network for Unsupervised Scene Conditioned Driving Behavior, </i> <br> Robotics: Science and Systems IDA workshop <strong>(RSS workshop)</strong>, 2020. </p>
						<a href="https://drive.google.com/file/d/16BNRuJv9PkPkHuwRMZVU-3CmUrdff-jJ/view">[manuscript]</a>
						<a href="">[code coming soon]</a>
						<a href="https://www.youtube.com/watch?v=hEPLdiiSNWQ">[talk]</a>
						<a href="https://www.youtube.com/watch?v=VJTXfTFuqxs">[demo]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/SLBO.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Yuping Luo<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Yuanzhi Li, Yuandong Tian, Tengyu Ma. <br> <i>Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees, </i> <br> International Conference on Learning Representation <strong>(ICLR)</strong>, 2019. </p>
						<a href="https://arxiv.org/abs/1807.03858">[arXiv]</a>
						<a href="https://github.com/facebookresearch/slbo">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/DPG.jpeg" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Hang Gao<sup>*</sup>, <b>Huazhe Xu</b><sup>*</sup>, Qi-zhi Cai, Ruth Wang, Fisher Yu, Trevor Darrell. <br> <i>Disentangling Propagationand Generation for Video Prediction, </i> <br> International Conference on Computer Vision <strong>(ICCV)</strong>, 2019. </p>
						<a href="https://arxiv.org/abs/1812.00452">[arXiv]</a>
						<a href="">[code coming soon]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/imperfect_demo.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Yang Gao<sup>*</sup>,<b>Huazhe Xu</b><sup>*</sup>, Fisher Yu, Sergey Levine, Trevor Darrell. <br> <i>Reinforcement Learning from Imperfect Demonstrations, </i> <br> Neurips 2018 Deep RL Symposium <strong>(Neurips Symposium)</strong>, 2018. </p>
						<a href="https://arxiv.org/abs/1802.05313">[arXiv]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/modularsc2.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Haoran Tang<sup>*</sup>, Dennis Lee<sup>*</sup>, Jeffrey O Zhang, <b>Huazhe Xu</b>, Trevor Darrell, Pieter Abbeel. <br> <i>Modular Architecture for StarCraft II with Deep Reinforcement Learning, </i> <br> he 14th AAAI Conference on Artificial Intelligenceand Interactive Digital Entertainmen <strong>(AIIDE)</strong>, 2018. </p>
						<a href="https://arxiv.org/abs/1811.03555">[arXiv]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/eedriving.gif" >
						</figure>
					</div>
					<div class="col-md-8">
						<p><b>Huazhe Xu</b><sup>*</sup>, Yang Gao<sup>*</sup>, Fisher Yu, Trevor Darrell. <br> <i>End-to-end Learning of Driving Models from Large-scale Video Datasets, </i> <br> Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2017. (<strong style="color:red">oral</strong>) </p>
						<a href="https://arxiv.org/abs/1612.01079">[arXiv]</a>
						<a href="https://github.com/gy20073/BDD_Driving_Model">[code]</a>
					</div>
				</div>

				<div class="row">
					<div class="media col-md-3">
						<figure class="pull-left">
							<img class="media-object img-rounded img-responsive"  src="images/pub-pic/nlor.png" >
						</figure>
					</div>
					<div class="col-md-8">
						<p>Ronghang Hu, <b>Huazhe Xu</b>, Marcus Rohrbach, Jiashi Feng, Kate Saeko, Trevor Darrell. <br> <i>atural Language Object Retrieval, </i> <br> Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, 2016. (<strong style="color:red">oral</strong>) </p>
						<a href="https://arxiv.org/abs/1511.04164">[arXiv]</a>
						<a href="https://github.com/ronghanghu/natural-language-object-retrieval">[code]</a>
					</div>
				</div>

			</div>
		</div>

		<footer>
			<span>&copy;2021 <b>Huazhe Xu</b> with thanks to <a href="http://bootsnipp.com/" rel="nofollow">Bootsnipp</a>.</span>
    	</footer>

	</div>

</body>
</html>
