<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Huazhe(Harry) Xu's Homepage</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="stylesheets/bootstrap.min.css">
    
    <meta name="viewport" content="width=device-width">
    
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Huazhe(Harry) Xu</h1>
		    <img src="images/me.jpeg"></img>
		<br />
		<br />
        <p>  <a href="http://tts.imtranslator.net/XMo1">How to pronounce my name?</a><br />
		In the Wade-Giles system of romanization, it is rendered as Huache Tsu.<br />
		In Chinese characters, it is 许华哲.</p>
        <p class="view"><a href="mailto:xuhuazhe12@gmail.com">xuhuazhe12@gmail.com</a></p>
        <p class="view"><a href="./Huazhe_Xu_CV.pdf">Download my CV</a></p>
	<p class="view"><a href="https://scholar.google.com/citations?user=t9HPFawAAAAJ&hl=en">google scholar</a></p>
      </header>


      <section>
	    <h2>About Me</h2>
			<p>I am a 4th year Ph.D student in Berkeley AI Research (BAIR) advised by Prof. Trevor Darrell. I received my bachelor degree from Tsinghua University. I have also spent  wonderful time at research labs of Facebook AI, University of Toronto and International Computer Science Institute enjoying the collaboration with Dr. Roberto Calandra, Prof. Tengyu Ma, Dr. Yuandong Tian, Prof. Jiashi Feng, Prof. Sergey Levine, Prof. Sanja Fidler and Prof. Raquel Urtasun.
               
			<p>My research focuses on modeling the dynamics of the world, leveraging/finding human priors for policy learning, and further enabling learning algorithms to learn in a sample-efficient manner. I am also interestded in solving complex video games/real applications with deep learning and reinforcement learning. </p>
			<p>I am also an amateur pianist and actively looking for potential collaborations (both music-wise or research-wise!). If you do reinforcement learning or computer vision projects or you play the piano, the violin or the cello, etc, feel free to contact me for some potential projects or some fun!</p>
        <h2>Education</h2>
		
        <div class="media">
            <span class="pull-left"><img src="./images/tsinghua.png" width="96px" height="96px"/></span>
            <div class="media-body">
                <p><span style="font-weight: bold">Aug. 2012 - Jul. 2016 </span>, Department of Electronic Engineering, <i><b>Tsinghua University</b></i>,</p>
                <p>Balchlor of Engineering, <b>GPA: 93/100, ranking: 5/238</b>.</ br> Average of Math and Math-Related Courses: <b>95.4/100</b>.</p>
            </div>  
        </div>

        <div class="media">
            <span class="pull-left"><img src="./images/toronto.png" width="96px" height="96px"/></span>
            <div class="media-body">
                <p><span style="font-weight: bold">Aug. 2014 - Dec. 2014</span>, School of Electrical and Computer Engineering, <i><b>University of Toronto</b></i>,</p>
                <p>Exchange Student, <b>GPA: 4.0/4.0</b>.</p>
            </div>
        </div>

        <div class="media">
            <span class="pull-left"><img src="./images/berkeley.png" width="96px" height="96px"/></span>
            <div class="media-body">
                <p><span style="font-weight: bold">July. 2015 - Sept. 2015, Aug. 2016 - now </span>, Department of Electrical and Computer Engineering, <i><b>University of California, Berkeley</b></i>,</p>
                <p>Visiting Researcher, PhD Student.</p>
            </div>
        </div>
              

        <h2>Selected Research Projects</h2>

          <h3>Modeling Visual Dynamics</h3>
          <div class="media">
            <span class="pull-left"><img src="./images/ilfo.jpeg" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Composable Semi-parametric Motion Generation </b>,</p>
                <p>Joint work w/ J. Xu T. Darrell </p>
                In submission [pdf] [code]
            </div>
        </div>
        <div class="media">
            <span class="pull-left"><img src="./images/OCVP.jpeg" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Object-Centric Video Prediction</b>,</p>
                <p>Joint work w/ B. Chen, M. Yang, Y. Gao, T. Darrell </p>
                Ongoing project [pdf] [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
        </div>
        <div class="media">
            <span class="pull-left"><img src="./images/DPG.jpeg" width="200px" height="100px"/></span>
            <div class="media-body">
				<p><b>Disentangling Propagation and Generation for Video Prediction</b>,</p>
				<p>Joint work w/ H. Gao, Q. Cai, R. Wang, F. Yu, T. Darrell</p>
                To appear in ICCV'19 <a href="https://arxiv.org/pdf/1812.00452.pdf">[pdf]</a> [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
					The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
					We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
					Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
        </div>


      <h3>Human priors and dynamics models for efficient policy learning</h3>
	  <div class="media">
            <span class="pull-left"><img src="./images/concept-0923.png" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Scoring-Aggregatin-Planning: framework for learning task-agnostic priors from interactions and rewards for zero-shot generalization</b>,</p>
                <p>Joint work w/ B. Chen, Y. Gao,  T. Darrell</p>
                AAAI'20 Genplan Workshop <font color="red">spotlight</font> [pdf] [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>
	
          <div class="media">
            <span class="pull-left"><img src="./images/multiagent.jpeg" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Model-based Multiagent Reinforcement Learning for Fast Adaptation</b>,</p>
                <p>Joint work w/ T. Zhang, Y. Tian,  T. Darrell</p>
                Ongoing Project [pdf] [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>
          <div class="media">
            <span class="pull-left"><img src="./images/ilns.png" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling</b>,</p>
                <p>Joint work w/ Y. Luo and T. Ma </p>
                ICLR 2020, also published at Neurips 2019 Deep Reinforcement Learning Workshop <a href="https://arxiv.org/abs/1907.05634">[pdf]</a> [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>

          <div class="media">
            <span class="pull-left"><img src="./images/SLBO.png" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Algorithmic Framework for Model-based Deep Reinforcement learning with Theoretical Guarantees</b>,</p>
                <p>Joint work w/ Y. Luo, Y. Li, Y. Tian, T. Darrell, T. Ma </p>
                published at International Conference on Learning Representations 2019 <a href="https://arxiv.org/pdf/1807.03858.pdf">[pdf]</a> <a href="https://github.com/facebookresearch/slbo">[code]</a> <a href="https://sites.google.com/view/algombrl/home">[website]</a>
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>

          <div class="media">
            <span class="pull-left"><img src="./images/imperfect_demo.png" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>Reinforcement Learning from Imperfect Demonstrations</b>,</p>
                <p>Joint work w/ Y. Gao, J. Lin, F. Yu, S. Levine and T. Darrell </p>
                Neurips Deep RL Symposium <a href="https://arxiv.org/pdf/1802.05313.pdf">[pdf]</a> [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>


    <h3>Learning policies for Real-world Applications</h3>
        <div class="media">
            <span class="pull-left"><img src="./images/eedriving.png" width="200px" height="100px"/></span>
            <div class="media-body">
                <p><b>End-to-End Learning of Driving Models from Large-scale Video Datasets</b>,</p>
                <p>Joint work w/ Y. Gao, F. Yu and T. Darrell </p>
                published at Conference on Computer Vision and Pattern Recognition 2017 (oral) <a href="https://arxiv.org/pdf/1612.01079.pdf">[pdf]</a> <a href="https://github.com/gy20073/BDD_Driving_Model">[code]</a>
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>
        <div class="media">
            <span class="pull-left"><img src="./images/modularsc2.png" width="200px" height="150px"/></span>
            <div class="media-body">
                <p><b>Modular Architecture for StarCraft II with Deep Reinforcement Learning</b>,</p>
                <p>Joint work w/ D. Lee, H. Tang, J. Zhang, T. Darrell and P. Abbeel </p>
                published at AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment 2018 <a href="https://arxiv.org/pdf/1811.03555.pdf">[pdf]</a> [code]
                <!-- <p>In this work,  we consider a challenging problem of automatically parsing scene images with only weakly-supervised information. </ br>
                    The core technique we develop to solve this problem  is a novel deep convolutional network architecture built on the fully convolutional network (FCN) model -- termed as category-attribute mutually boosted FCN (CAM-FCN).
                    We formulate the problem of updating or training CAM-FCN  with non-linear global constraints  into an alternative optimization problem through decoupling the updating procedure of two sub-FCNs.
                    Extensive experiments on benchmark datasets clearly demonstrate the superior effectiveness of CAM-FCN over well-established baselines for the weakly supervised scene parsing task.</p> -->
            </div>
          </div>





    <h3>Past Projects</h3>
       <div class="media">
            <span class="pull-left"><img src="./images/proj2.png" width="200px" height="200px"/></span>
            <div class="media-body">
				<p><b>Natural Language and Object Retrieval</b>,</p>
				<p>Joint work w/ R. Hu, M. Rohrbach, J. Feng, K. Saenko and T. Darrell,</p>
                published at Conference on Computer Vision and Pattern Recognition 2016 (oral) <a href="https://arxiv.org/pdf/1511.04164.pdf">[pdf]</a> <a href="https://github.com/ronghanghu/natural-language-object-retrieval">[code]</a>
                <!-- <p>We address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object.
				Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context.
				To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval,
				integrating spatial conﬁgurations and global scene-level contextual information into the net-work.</p> -->
            </div>
        </div>

		<div class="media">
            <span class="pull-left"><img src="./images/proj3.png" width="200px" height="100px"/></span>
            <div class="media-body">
				<p><b>Automobile Visual Taste Ranking</b>,</p>
				<p>Joint work w/ S. Fidler, R. Urtasun,</p>
                <p>2015 Fall , <i><b>University of Toronto</b></i>,</p>
                <!-- <p>This project explores with issue of human's visual taste on automobiles.
					We find that most recommendation systems lack of personality and customized service. 
					We utilize image visual features and well-established survey questionaire with ranksvm and CNN techniques to detect personalized visual preference for customer. </p> -->
            </div>
        </div>
		
        <h2>Publications and Manuscripts</h2>
		<!--<h4>Conference Paper</h4>-->
		<ul>
	    <li><p><b>R. Yang</b>，<b>H. Xu</b>, Y. Wu, X. Wang, 
Multi-Task Reinforcement Learning with Soft Modularization, Arxiv Preprint, </i>. <a href="https://arxiv.org/pdf/2003.13661.pdf">[pdf]</a> </li>
		</ul>
		<ul>
	    <li><p><b>H. Xu<sup>*</sup></b>， B. Chen<sup>*</sup>, Y. Gao, T. Darrell, 
Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions and sparse rewards for zero-shot generalization, Arxiv Preprint, </i>. <a href="https://arxiv.org/abs/1910.08143">[pdf]</a> </li>
		</ul>
		<ul>
	    <li><p>Y. Luo, <b>H. Xu</b>, T. Ma, 
Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling,, Arxiv Preprint, </i>. <a href="https://arxiv.org/abs/1907.05634">[pdf]</a> </li>
		</ul>
		<ul>
	    <li><p>H. Gao<sup>*</sup>, <b>H. Xu<sup>*</sup></b>, Q. Cai, R. Wang, F. Yu, T. Darrell, 
Disentangling Propagation and Generation for Video Prediction, To Appear in ICCV'19, </i>. <a href="https://arxiv.org/pdf/1812.00452.pdf">[pdf]</a> </li>
		</ul>
		<ul>
	    <li><p>H. Tang<sup>*</sup>, D. Lee<sup>*</sup>, J. Zhang,  <b>H. Xu</b>, T. Darrell, P. Abbeel, Modular Architecture for StarCraft II with Deep Reinforcement Learning. The 14th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE'18)</i>. <a href="https://arxiv.org/pdf/1811.03555.pdf">[pdf]</a> </li>
		</ul>
		
		<ul>
	    <li><p>Y. Luo<sup>*</sup>, <b>H. Xu<sup>*</sup></b>, Y. Li, Y. Tian, T. Darrell, T. Ma, Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees <i>.ICLR'19 (also appeared as ICMLW'18, NeurIPSW'18)</i>. <a href="https://arxiv.org/pdf/1807.03858.pdf">[pdf]</a> </li>
		</ul>
			
		<ul>
	    <li><p>Y. Gao<sup>*</sup>, <b>H. Xu<sup>*</sup></b>, F. Yu, S. Levine, T. Darrell, Reinforcement Learning from Imperfect Demonstrations<i>. in NIPS Deep RL Syposium</i>. <a href="https://arxiv.org/pdf/1802.05313.pdf">[pdf]</a></li>
		</ul>
	
		<ul>	
	    <li><p><b>H. Xu<sup>*</sup></b>, Y. Gao<sup>*</sup>, F. Yu, T. Darrell, End-to-end Learning of Driving Models from Large-scale Video Datasets.
                <i>in CVPR 2017 (<font color="red">oral</font>)</i>. <a href="https://arxiv.org/abs/1612.01079">[pdf]</a></p></li> 
        	</ul>
			
		<ul>
            <li><p>R. Hu, <b>H. Xu</b>, M. Rohrbach, J. Feng, K. Saenko, T. Darrell, Natural Language Object Retrieval.
                <i>in CVPR 2016 (<font color="red">oral</font>)</i>. <a href="http://arxiv.org/pdf/1511.04164.pdf">[pdf]</a></p></li> 
        	</ul>
	
		<ul>
			<li><p>Tian Xie, Qian Han, <b>Huazhe Xu</b>, Zihao Qi, Wenqian Shen. A Low-Complexity Linear Precoding Scheme Based on
SOR Method for Massive MIMO Systems. <i>Vehicular Technology Conference (VTC Spring), 2015 IEEE 81st ,
vol., no., pp.1-5, 11-14 May 2015</i>. <a href="">[pdf]</a></p></li>
		</ul>
	
		
        <h2>Honors and Awards</h2>
		<ul>
			<li><p>EECS Excellence Award, UC Berkeley, 2016</p></li>
			<li><p>Comprehensive Excellent Scholarship, Tsinghua Univ, 2013</p></li>
			<li><p>Academic Excellent Scholarship, Tsinghua, Univ, 2014</p></li>
			<li><p>Academic Excellent Scholarship, Tsinghua, Univ, 2015</p></li>
		</ul>

	<h2>News!</h2>
	<ul>
                        <li><p>Dec, 2019: One paper accepted by ICLR'20. </p></li>
                </ul>
	<ul>
                        <li><p>July, 2019: One paper accepted by ICCV'19. </p></li>
                </ul>
        <ul>
			<li><p>May, 2019: One paper accepted by ICLR'19. </p></li>
		</ul>
		<ul>
			<li><p>Nov 19, 2018, Our Starcraft 2 Project is covered by Synced (机器之心) and <a href=https://jack-clark.net/2018/11/19/import-ai-121-sony-researchers-make-ultra-fast-imagenet-training-breakthrough-berkeley-researchers-tackle-starcraft-ii-with-modular-rl-system-and-germany-adds-e3bn-for-ai-research/>Import AI</a>. </p></li>
		</ul>
		<ul>
			<li><p>Dec 18, 2017: Giving a talk at Berkeley DeepDrive Workshop. </p></li>
		</ul>
		<ul>
			<li><p>Oct 31, 2017: Giving a talk at <a href=https://sites.google.com/view/visionseminar>MIT vision seminar</a> in Boston. </p></li>
		</ul>
		<ul>
			<li><p>June 1, 2017: Giving a talk in at Tsinghua University in Beijing.</p></li>
		</ul>
		<ul>
			<li><p>Our paper 'End-to-end Learning of Driving Model from Largescale Dataset' appeared at 'New_era' Wechat Media. </p></li>
		</ul>
		<ul>
			<li><p>Interviewed by a 'Leiphone' reporter about BAIR Blog.</p></li>
		</ul>
	<h2>Service</h2>

		<ul>
			<li><p>Committee member of Deep RL workshop at Neurips 2019 </p></li>
		</ul>
		<ul>
			<li><p>Reviewer of ICLR, Neurips, CVPR, ICML, ACCV, ICCV </p></li>
		</ul>
		<ul>
			<li><p>Reviewer of NIPS'17 Intelligent Transportation Workshop</p></li>
		</ul>
		<ul>
			<li><p>Reviewer of IEEE Transactions on Multimedia’ 16, 17</p></li>
		</ul>
		<ul>
			<li><p>Editor in BAIR Blog Editorial Board, manager of BAIR facebook handle.</p></li>
		</ul>
		
        <h2>Miscellaneous</h2>
		<ul>
	    <li><p><a href="https://www.goodreads.com/book/show/23422.The_Art_of_Travel?ac=1&from_search=true"> Alain De Botton: The Art of Travel</a> (I traveled less often after reading this :p)</p></li>
            <li><p><a href="http://www.cs.toronto.edu/~fidler/CSC420.html">Introduction to Image Understanding</a> (A lovely Course from Prof. Sanja Fidler)</p></li>
            <li><p><a href="">Hilary Hahn's performance of Chaconne</a> (The piece that moves me. I am waiting for Hilary's next record of this piece years later.)</p></li>
	    <li><p><a href="https://www.youtube.com/watch?v=lncNcNtGkJY">Franz Schubert Sonata D960</a> (Kafka on The Shore novel by Murakami brought Schubert's D850 to me when I was 16. Then there started my love for Schubert's music.)</p></li>
		</ul>
	
	<h2>Attempts</h2>
		Here is a partial catalog of my attempts at becoming more than what I am today. I doubt I will ever succeed totally but I hope I will never stop trying.
		<ul>
			<h4>Clarinet Project</h4> Due to COVID-19, I don't have access to a piano. I started polishing my rusty clarinet skills. 
		</ul>
		<ul>
	     		<h4>Piano Project</h4> I am now learning to play the piano(again!) with pianist <a href="http://annerainwater.com/">Anne Rainwater</a>. Here are some pieces I enjoy playing recently: 
			<p style="text-indent:2em;"> Prelude op. 23 no. 5 in g minor, Sergei Rachmaninov</p>
			<p style="text-indent:2em;"> Etude op. 10 No. 12 in C minor "Revolutionary", Frédéric Chopin</p>
			<p style="text-indent:2em;"> Piano Sonata No. 15 in D major, Op. 28, Ludwig van Beethoven</p>
			<p style="text-indent:2em;"> Three Romances for Violin and Piano Op. 22, Clara Schumann</p>
            		<p style="text-indent:2em;"> Phatasiestucke, Robert Schumann</p>
            		<p style="text-indent:2em;"> Piano Sonata No. 3 in C major, Op. 2, No. 3, Ludwig van Beethoven</p>
			<p style="text-indent:2em;"> Impromptu in B-flat Major D. 935, No. 3 (Op. 142), Franz Schubert </p>
			<p style="text-indent:2em;"> French Suites, BWV 817, Johann Sebastian Bach</p>
			<p style="text-indent:2em;"> Prelude and Fugue No.2, Johann Sebastian Bach</p>
			<p style="text-indent:2em;"> 4 Improptus, D899, Franz Schubert</p>
			<p style="text-indent:2em;"> kinderszenen op. 15, Robert Schumann</p>
			<p style="text-indent:2em;"> Waltz in D-flat major, Op. 64, No. 1, Valse du petit chien, Frédéric Chopin</p>
			<p style="text-indent:2em;"> Grande valse brillante in E-flat major, Op. 18,  Frédéric Chopin</p>
			<p style="text-indent:2em;"> Tempest Sonata, Ludwig van Beethoven</p>
		</ul>
		<ul>
			<h4>Reading Project</h4> I enjoy reading sci-fi novels and love stories. Recently finished Isaac Asimov's The Greater Foundation Series and The Gods Themselves. I also enjoy the books from Italo Calvino, Xiaobo Wang, Virginia Woolf, Haruki Murakami, Cixin Liu, etc.
		</ul>
		<ul>
			<h4>Tennis</h4> I also enjoy playing tennis and running. If you are visiting Berkeley and want to play or run, ping me on email.
		</ul>
	
      	<h2>Who searched for me!</h2>
		<br class="clearfix" />
                <div id="clustrmaps-widget"></div>
                <!--<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=EvgNAh22dvx9JCt-FQ-UEUQ06TU2qe0ifBmBc0890K4"></script> -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=c2bfbf&w=a&d=mHndNXFUvtHvMdYMimBeCXb3NumiTYkNi8xKB-x2D2Q&co=ffffff&cmo=ff5353&cmn=ff5353&ct=000000'></script>
            </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
